{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.safaribooksonline.com/library/view/natural-language-text/9781491976487/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'cześć'\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(a, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.encode('ascii', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.encode('utf-8', errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr. Smith loves tacos.', 'He has a Ph.D. in tacology']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"Mr. Smith loves tacos. He has a Ph.D. in tacology\"\n",
    "\n",
    "sents = nltk.sent_tokenize(text)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'chicken', 'danced', 'because', 'she', 'loved', 'disco']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The chicken danced because she loved disco\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\Artur_Zahreba\\\\AppData\\\\Local\\\\Continuum\\\\Anaconda3\\\\lib\\\\site-packages\\\\setuptools-27.2.0-py3.6.egg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-259884d460cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdeprecated\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresolve_model_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\spacy\\deprecated.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\spacy\\cli\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpackage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\spacy\\cli\\download.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlink_package\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\spacy\\cli\\link.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mpip\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInstallationError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCommandError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPipError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mpip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_installed_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_prog\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist_is_editable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvcs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmercurial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbazaar\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mwrite_delete_marker_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 27\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mpip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vendor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vendor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vendor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPY2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3018\u001b[0;31m \u001b[1;33m@\u001b[0m\u001b[0m_call_aside\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3019\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_initialize_master_working_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m     \"\"\"\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_call_aside\u001b[0;34m(f, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[1;31m# from jaraco.functools 1.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_call_aside\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3004\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3005\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_initialize_master_working_set\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3029\u001b[0m     \u001b[0mat\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mown\u001b[0m \u001b[0mrisk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3030\u001b[0m     \"\"\"\n\u001b[0;32m-> 3031\u001b[0;31m     \u001b[0mworking_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWorkingSet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_master\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3032\u001b[0m     \u001b[0m_declare_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworking_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_build_master\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0mPrepare\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmaster\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \"\"\"\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0m__main__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__requires__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, entries)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_entry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36madd_entry\u001b[0;34m(self, entry)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m         \u001b[1;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfind_distributions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mfind_eggs_in_zip\u001b[0;34m(importer, path_item, only)\u001b[0m\n\u001b[1;32m   1947\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEggMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimporter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1949\u001b[0;31m     \u001b[1;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PKG-INFO'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1950\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0monly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mhas_metadata\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhas_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0megg_info\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0megg_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_has\u001b[0;34m(self, fspath)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_has\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfspath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0mzip_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zipinfo_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mzip_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzipinfo\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mzip_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_isdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfspath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mzipinfo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1701\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mzipinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zip_manifests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marchive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_resource_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Artur_Zahreba\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \"\"\"\n\u001b[1;32m   1642\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m         \u001b[0mmtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmtime\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmtime\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\Artur_Zahreba\\\\AppData\\\\Local\\\\Continuum\\\\Anaconda3\\\\lib\\\\site-packages\\\\setuptools-27.2.0-py3.6.egg'"
     ]
    }
   ],
   "source": [
    "from spacy import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-63b4325d4424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "parser(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4f4ff20525ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;33m[\u001b[0m\u001b[0mtkn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morth_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtkn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtoekn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morth_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "[tkn.orth_ for tkn in tokens if not tkn.orth_.isspace()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'which', 'over', 'then', 'some', 'will', 're', 'themselves', 'by', 's', 'yourselves', 'did', 'hasn', 'own', 'can', 've', 'only', 'is', 'of', 'up', 'ours', 'be', 't', 'as', 'should', 'again', 'd', 'am', 'and', 'didn', 'hers', 'no', 'so', 'off', 'aren', 'were', 'them', 'nor', 'won', 'him', 'that', 'until', 'because', 'shouldn', 'out', 'm', 'you', 'what', 'through', 'ain', 'than', 'do', 'those', 'before', 'from', 'theirs', 'been', 'why', 'such', 'needn', 'don', 'not', 'too', 'our', 'being', 'these', 'shan', 'now', 'both', 'himself', 'was', 'yourself', 'his', 'in', 'my', 'me', 'he', 'there', 'they', 'while', 'once', 'further', 'to', 'where', 'we', 'very', 'it', 'the', 'mightn', 'll', 'does', 'against', 'any', 'yours', 'had', 'under', 'when', 'i', 'other', 'here', 'who', 'below', 'couldn', 'y', 'their', 'a', 'most', 'same', 'between', 'hadn', 'after', 'wouldn', 'an', 'have', 'myself', 'just', 'or', 'down', 'more', 'all', 'this', 'doing', 'with', 'about', 'above', 'whom', 'during', 'few', 'how', 'its', 'weren', 'at', 'haven', 'your', 'has', 'ma', 'are', 'but', 'mustn', 'her', 'if', 'each', 'doesn', 'wasn', 'for', 'herself', 'into', 'isn', 'itself', 'o', 'having', 'on', 'she', 'ourselves'}\n"
     ]
    }
   ],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chicken', 'danced', 'loved', 'disco']\n"
     ]
    }
   ],
   "source": [
    "content = [tkn for tkn in tokens if not tkn.lower() in stopwords]\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Vector Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "term freaquency (vector: number of times the word occurs) \n",
    "\n",
    "*\n",
    "\n",
    "inverse document frequency (log(total number of docs / number of docs containing term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"I love tacos.\",\n",
    "    \"I don't choose to take a nap . The nap chooses me.\",\n",
    "    \"That man is nice as pie with ice cream.\",\n",
    "    \"This pizza is an addront to nature.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'love', 'tacos', '.'],\n",
       " ['I',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'choose',\n",
       "  'to',\n",
       "  'take',\n",
       "  'a',\n",
       "  'nap',\n",
       "  '.',\n",
       "  'The',\n",
       "  'nap',\n",
       "  'chooses',\n",
       "  'me',\n",
       "  '.'],\n",
       " ['That', 'man', 'is', 'nice', 'as', 'pie', 'with', 'ice', 'cream', '.'],\n",
       " ['This', 'pizza', 'is', 'an', 'addront', 'to', 'nature', '.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_docs = [word_tokenize(text) for text in documents]\n",
    "gen_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I\n",
      "1 love\n",
      "2 tacos\n",
      "3 .\n",
      "4 do\n",
      "5 n't\n",
      "6 choose\n",
      "7 to\n",
      "8 take\n",
      "9 a\n",
      "10 nap\n",
      "11 The\n",
      "12 chooses\n",
      "13 me\n",
      "14 That\n",
      "15 man\n",
      "16 is\n",
      "17 nice\n",
      "18 as\n",
      "19 pie\n",
      "20 with\n",
      "21 ice\n",
      "22 cream\n",
      "23 This\n",
      "24 pizza\n",
      "25 an\n",
      "26 addront\n",
      "27 nature\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs) # maps every word to a number\n",
    "num_words = len(dictionary)\n",
    "for idx, word in dictionary.items():\n",
    "    print(idx, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n",
      "to\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7316fce35622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# convert string to token id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ran'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "# convert tkn id to string\n",
    "print(dictionary[7])\n",
    "print(dictionary.id2token[7])\n",
    "\n",
    "# convert string to token id\n",
    "print(dictionary.token2id('ran'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-3d6e8f6534b1>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-3d6e8f6534b1>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    bow_doc = dictionary.doc2bow(['I', 'love', 'love', 'love', 'tacos, '!'])\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# create a bag of words = tf\n",
    "# ! is not listed because it's not in the dictionary\n",
    "bow_doc = dictionary.doc2bow(['I', 'love', 'love', 'love', 'tacos, '!'])\n",
    "print(bow_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(0, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1)], [(3, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)], [(3, 1), (7, 1), (16, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# create corpus = list of bows\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=4, num_nnz=34)\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "# numnnz - number of tokens in the model\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'tacos', '.']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n",
      "[(0, 0.3333333333333333), (1, 0.6666666666666666), (2, 0.6666666666666666)]\n"
     ]
    }
   ],
   "source": [
    "print(gen_docs[0])\n",
    "print(corpus[0])\n",
    "print(tf_idf[corpus][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob is a text processing libray for Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://textblob.readthedocs.io/en/dev/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The machine learning functions in TextBlob wrap NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/sloria/TextBlob \n",
    "\n",
    "https://github.com/nltk/nltk/tree/develop/nltk/classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/nltk/nltk/blob/develop/nltk/classify/naivebayes.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'description', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "texts = fetch_20newsgroups(subset='train')\n",
    "dir(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "[7 4 4 ..., 3 1 8]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(len(texts.target))\n",
    "print(texts.target)\n",
    "print(texts.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 alt.atheism\n",
      "1 comp.graphics\n",
      "2 comp.os.ms-windows.misc\n",
      "3 comp.sys.ibm.pc.hardware\n",
      "4 comp.sys.mac.hardware\n",
      "5 comp.windows.x\n",
      "6 misc.forsale\n",
      "7 rec.autos\n",
      "8 rec.motorcycles\n",
      "9 rec.sport.baseball\n",
      "10 rec.sport.hockey\n",
      "11 sci.crypt\n",
      "12 sci.electronics\n",
      "13 sci.med\n",
      "14 sci.space\n",
      "15 soc.religion.christian\n",
      "16 talk.politics.guns\n",
      "17 talk.politics.mideast\n",
      "18 talk.politics.misc\n",
      "19 talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(i, texts.target_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', ':', 'lerxst', '@', 'wam.umd.edu', '(', 'where', \"'s\", 'my', 'thing', ')', 'subject', ':', 'what', 'car', 'is', 'this', '!', '?', 'nntp-posting-host', ':', 'rac3.wam.umd.edu', 'organization', ':', 'university', 'of', 'maryland', ',', 'college', 'park', 'lines', ':', '15', 'i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', '.', 'it', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'it', 'was', 'called', 'a', 'bricklin', '.', 'the', 'doors', 'were', 'really', 'small', '.', 'in', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'this', 'is', 'all', 'i', 'know', '.', 'if', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.', 'thanks', ',', '-', 'il', '--', '--', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst', '--', '--']\n"
     ]
    }
   ],
   "source": [
    "# TextBlob automatically tokenizes\n",
    "# https://github.com/sloria/TextBlob/blob/dev/textblob/classifiers.py\n",
    "from textblob.tokenizers import word_tokenize\n",
    "print(list(word_tokenize(texts.data[0].lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train = 10000\n",
    "num_test = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get vocabulary\n",
    "# Creating a vocab to limit the features\n",
    "# https://docs.python.org/2/library/collections.html#collections.Counter\n",
    "all_text = ''\n",
    "for i in range(num_train):\n",
    "    all_text += texts.data[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Make a list of tokenized words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(all_text)\n",
    "tokens = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>', ',', 'the', '.', '--', ':', 'to', '(', ')', \"'ax\", 'of', 'a', 'and', '@', 'i', 'in', 'is', 'that', '?', 'it']\n"
     ]
    }
   ],
   "source": [
    "# 3. Get Most common words\n",
    "import collections\n",
    "cnt = collections.Counter(tokens).most_common()\n",
    "vocab = [k for k, v in cnt if v >= 10]\n",
    "print(vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"from : @ wam.umd.edu ( where 's my thing ) subject : what car is this ! ? nntp-posting-host : organization : university of maryland , college park lines : 15 i was wondering if anyone out there could enlighten me on this car i saw the other day . it was a sports car , looked to be from the late early 70s . it was called a . the doors were really small . in addition , the front bumper was separate from the rest of the body . this is all i know . if anyone can a model name , engine specs , years of production , where this car is made , history , or whatever info you have on this looking car , please e-mail . thanks , - il -- -- brought to you by your neighborhood -- --\", 'rec.autos')\n"
     ]
    }
   ],
   "source": [
    "# 4. Create the training and testing data, filtering out words not in our vocabulary\n",
    "# Important! Each word is a feature, and you don't want too many features\n",
    "\n",
    "train_data = []\n",
    "for i in range(num_train):\n",
    "    tokens = word_tokenize(texts.data[i])\n",
    "    item_text = ' '.join([t.lower() for t in tokens if t.lower() in vocab])\n",
    "    train_data.append((item_text, texts.target_names[texts.target[i]]))\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"from : @ ( robert ) subject : re : sho and sc nntp-posting-host : organization : forest lane design center lines : 42 in article < @ constellation.ecn.uoknor.edu > callison @ uokmax.ecn.uoknor.edu ( james p. callison ) writes : > in article < @ ganglion.ann-arbor.mi.us > david @ ganglion.ann-arbor.mi.us ( david hwang ) writes : > > in article < @ unisql.uucp > wrat @ unisql.uucp ( wharfie ) writes : > > > in article < @ netcom.com > @ netcom.com ( chris ) writes : > > > > > why anyone would order an sho with an automatic transmission is > beyond me ; if you ca n't handle a stick , you should stick with a > regular taurus and leave the sho to real drivers . that is not to > say that there are n't real drivers who ca n't use the stick ( eg > disabled persons ) , but they are n't in any position to use an > sho anyway . > > i would be willing to bet that if we removed the automatic > transmissions from all `` '' cars ( like the > mustangs , , and the like ) we 'd cut down on the number of > accidents each year . autos are fine for little sedans , > but they have no business in performance cars , imho . > > james > i have to disagree with this . i have a 92 with a 350 and a auto w/ overdrive , and it is really better that way . chevy autos are for their long life and ability to handle amount of power . i live in the dallas area , and a manual would be much harder to drive in the traffic here . now if i still lived out in the sticks like i used to , a manual would be more fun . , an auto is less ... i would hate to have to be shifting gears while i was trying to ease into traffic in the freeways here . , i can hold my own against any stock 5.0 mustang or 5.0 camaro w/ a five speed . all of this imho ... : ) -- * robert l. * - infinity is a notion best * * @ * in a warm bed . * * texas instruments , inc. * - my opinions are my own , not ti 's . *\", 'rec.autos')\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "for i in range(num_train, num_train + num_test):\n",
    "    tokens = word_tokenize(texts.data[i])\n",
    "    item_text = ' '.join([t.lower() for t in tokens if t.lower() in vocab])\n",
    "    test_data.append((item_text, texts.target_names[texts.target[i]]))\n",
    "\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. Naive Bayes classifier re-parses the whole text of the corpus for each record\n",
    "# This makes it slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6. Show what the features look like and what the important ones are\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
